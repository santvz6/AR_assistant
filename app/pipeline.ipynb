{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7657e93c",
   "metadata": {},
   "source": [
    "# AR Assistant: Documentación del Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad051e5",
   "metadata": {},
   "source": [
    "En este Notebook describiremos el pipeline de nuestro Asistente de Realidad Aumentada (AR), el cual gestiona una interacción completa de inicio a fin, integrando tecnologías de procesamiento de voz (Speech-to-Text, Text-to-Speech) y visión por computadora (Detección de Objetos, Procesamiento de Vídeo y Q&A basado en Visión)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9976656",
   "metadata": {},
   "source": [
    "## Uso principal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c94792",
   "metadata": {},
   "source": [
    "El *pipeline* completo se ejecuta simplemente creando una instancia de la clase ARAssistant y llamándola.\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    assistant = ARAssistant()\n",
    "    assistant()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261add7",
   "metadata": {},
   "source": [
    "## Estructura del Pipeline ARAssistant.__call__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee0b46",
   "metadata": {},
   "source": [
    "### Inicio y Preparación\n",
    "Inicialmente esperamos la orden del usuario transcribiendo el audio de entrada (Whisper) para detectar comandos como [\"empezar\", \"juego\"]. Al detectarlo, explicamos el juego usando Text-to-Speech (TTS). Luego, procesamos el video del entorno con YOLO para detectar objetos. Seleccionamos un frame aleatorio y de allí elegimos un único objeto secreto con una ponderación basada en el accuracy de cada detección."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7aca33",
   "metadata": {},
   "source": [
    "### Bucle de Juego (Q&A)\n",
    "Una vez procesado el vídeo iniciamos el bucle Q&A simulando una secuencia de audio lineal. Por cada pregunta, la transcribimos y verificamos si el texto contiene el nombre del objeto secreto para una victoria inmediata. Si no acierta, almacenamos la pregunta en una lista.\n",
    "\n",
    "**Generación de Respuesta:** Cada dos preguntas, usamos el modelo CLIP para comparar la imagen del objeto con las preguntas almacenadas. La pregunta con la mayor accuracy (similitud) se selecciona como la respuesta más relevante, la cual reproducimos al usuario.\n",
    "\n",
    "**Generación de Pista:** Cada seis preguntas (cada tres pares), generamos una pista. Elegimos un atributo predefinido (\"color\", \"forma\", ...) y usamos CLIP para relacionar la imagen con las posibles descripciones de ese atributo. La descripción con mayor probabilidad se da como pista al usuario, incluyendo el porcentaje de confianza.\n",
    "\n",
    "\n",
    "```python\n",
    "def __call__(self):\n",
    "        # pipline\n",
    "        if not self.user_init.wait_for_init():\n",
    "            return\n",
    "        self.audio.speak(text=GAME_EXPLANATION, filename=\"explain_game.mp3\")\n",
    "        self.video.process_video(output_path=os.path.join(ENVIRONMENT_DIR, \"processed_video.mp4\"))\n",
    "        self.qna.start_loop(\n",
    "            detected_object_path=self.video.detected_object_path,\n",
    "            detected_object_name = self.video.detected_object_name\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724bee8",
   "metadata": {},
   "source": [
    "# Funcionalidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b7fee",
   "metadata": {},
   "source": [
    "## Funcionalidad 1\n",
    "\n",
    "**Conteo: \"¿Qué objetos hay en la imagen?\" → listar objetos detectados.**\n",
    "\n",
    "Lo implementamos en *VideoManager.process_video()*, iteramos sobre todos los frames muestreados del video y usamos el detector YOLO *self.detector.detect(frame)*. Luego almacenamos los nombres de todos los objetos únicos detectados en el set llamado *self.detected_object_names*. El número de objetos detectados, *detected_object_names_len*, lo utilizamos posteriormente para dar una pista específica en *QnAEngine.process_clue()*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e2c96",
   "metadata": {},
   "source": [
    "```log\n",
    "2025-12-12 10:38:42,134 - app_logger - DEBUG - QnAEngine: process_clue(): clue key: objects\n",
    "2025-12-12 10:38:42,134 - app_logger - DEBUG - QnAEngine: process_clue(): clue_text: Aquí tienes una pista, atento. A continuación te dire los 14 posibles objetos: televisión, florero, cuenco, cama, maleta, microondas, libro, cepillo de dientes, sofá, comedor, baño, reloj, silla, banco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad7a93",
   "metadata": {},
   "source": [
    "## Funcionalidad 2\n",
    "\n",
    "**Selección: \"Márcame el ``[objeto]``\" → dibujar bounding box o resaltado.**\n",
    "\n",
    "Lo implementamos en dos etapas:\n",
    "\n",
    "- **Detección y Anotación en Video:** Nuestro método *VideoManager.__annotate_frame()* se encarga de dibujar los bounding boxes y las etiquetas de clase/confianza sobre cada frame en el que se detecta un objeto. Luego, *VideoManager.process_video()* genera un video de salida con todos los frames anotados, simulando nuestra interfaz de AR.\n",
    "\n",
    "- **Guardado del Objeto Secreto:** En nuestro método *VideoManager.__save_select_detection()* seleccionamos un objeto de un frame al azar (elegido con pesos basados en la confianza de la detección) y guardamos una imagen recortada de ese objeto (el \"objeto secreto\") en *detected_object_path*. Esto simula el \"resaltado\" al aislar el objeto de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b2792",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"resources/images/detected_frame.png\" style=\"width:500px;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec22157",
   "metadata": {},
   "source": [
    "## Funcionalidad 3\n",
    "\n",
    "**Preguntas: \"¿Hay una ``[objeto]``?\"→ respuesta sí/no con evidencia visual.**\n",
    "\n",
    "Lo implementamos de forma avanzada, es decir, usamos el modelo **CLIP** para hacer **matching** entre la imagen del objeto detectado y las preguntas del usuario. En nuestro bucle de Q&A, la respuesta del sistema se basa en la similitud entre el texto de la pregunta (o las pistas) y la imagen, lo que equivale a responder si \"existe\" o \"se relaciona\" nuestra respuesta con la escena visual.\n",
    "\n",
    "También lo implementamos al analizar la respuesta del usuario, es decir, si el usuario ha acertado el objeto misterioso. En este caso estamos comparando si la respuesta corresponde con un objeto que estaba en un frame cocnreto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73165ed5",
   "metadata": {},
   "source": [
    "```log\n",
    "2025-12-12 10:35:16,594 - app_logger - DEBUG - QnAEngine: process_question(): question:  Es una silla.\n",
    "2025-12-12 10:35:16,594 - app_logger - DEBUG - QnAEngine: while question_idx < len(questions_files): success silla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d433170",
   "metadata": {},
   "source": [
    "## Funcionalidad 4\n",
    "\n",
    "**Matching CLIP texto-imagen → buscar lo más parecido a una palabra/frase.**\n",
    "\n",
    "Lo tenemos implementado utilizando el **CLIPProcessor** de forma intensiva tanto para generar respuestas, cada dos preguntas, como para dar pistas, cada seis preguntas. En ambos casos, el sistema busca la mayor correlación entre la imagen del objeto y una lista de textos para tomar una decisión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3032c36",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"resources/images/detected_object.png\" style=\"width:200px;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655bae73",
   "metadata": {},
   "source": [
    "```log\n",
    "2025-12-12 10:38:36,484 - app_logger - DEBUG - QnAEngine: process_question(): question 4\n",
    "2025-12-12 10:38:36,703 - app_logger - DEBUG - QnAEngine: process_question(): question:  ¿Cirbe para descansar?\n",
    "2025-12-12 10:38:36,703 - app_logger - DEBUG - QnAEngine: process_question(): question 5\n",
    "2025-12-12 10:38:36,886 - app_logger - DEBUG - QnAEngine: process_question(): question:  Es oscuro.\n",
    "2025-12-12 10:38:36,886 - app_logger - DEBUG - QnAEngine: process_answer(): answer 3\n",
    "2025-12-12 10:38:37,539 - app_logger - DEBUG - QnAEngine: process_answer(): answer: cirbe descansar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494aa77",
   "metadata": {},
   "source": [
    "## Funcionalidad 5\n",
    "\n",
    "**Vídeo (opcional con puntuación extra).**\n",
    "\n",
    "Mediante *VideoManager.process_video()* simulamos el comportamiento en tiempo real de unas gafas de realidad aumentada. Es decir por cada frame estamos analizando todos los objetos contenidos en él. Luego, realizamos una selección, de forma aleatoria, de un frame que contenga objetos. A partir de aquí, nuestro problema se resuelve a un probleam de imágenes.\n",
    "\n",
    "Es decir, estamos utilizando el vídeo para analizar el entorno y quedarnos con un frame concreto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ac590",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <video width=\"600\" controls>\n",
    "        <source src=\"resources/videos/environment/processed_video.mp4\" type=\"video/mp4\">\n",
    "        Tu navegador no soporta video.\n",
    "    </video>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
